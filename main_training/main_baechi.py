import sys, os
sys.path.append("..")

import argparse
import torch
from utils import logger
import time
from torch import optim, nn
import numpy as np

import config

from model_library.model_nursery import get_model

import baechi_core.baechi_profiler as profiler 
import baechi_core.baechi_graph as baechi_graph
import baechi_core.util_functions as utilsf

from baechi_core.placer.m_etf import m_etf
from baechi_core.placer.m_sct import m_sct
from baechi_core.placer.m_topo import m_topo
import baechi_core.assigners as assigners 

_LOGGER = logger.get_logger(__file__, level=logger.INFO)
config.MAX_DEVICE_COUNT = torch.cuda.device_count()

## Typical runs:
## python main_baechi.py (runs default settings in config.py)
## python main_baechi.py -m inception_v3 -sch sct -n 50 -b 32 -Ng 2

#---------------------------------------- Process Input Arguments ------------------------------------------
parser = argparse.ArgumentParser(description='Baechi PyTorch Training')

def check_positive_float(value):
    ivalue = float(value)
    if ivalue <= 0:
        raise argparse.ArgumentTypeError("%s is an invalid positive float value" % value)
    return ivalue

## Experiment Setup
parser.add_argument('--run-type', default=config.RUNTYPE , type=str, help='Type of run: training or inference')

parser.add_argument('-Ng', '--gpu-num', default=config.NUM_GPUS, type=int, choices=range(1,config.MAX_DEVICE_COUNT+1), 
                        metavar='N', help='Number of GPU to use. Max value is torch.cuda.device_count()')

## Profiler Settings
parser.add_argument('-Pr', '--prof-rounds', default=config.PROFILING_ROUNDS, type=int,
                        metavar='P', help='no. of rounds the profiler runs & averages the profiles (default: 20)')

parser.add_argument('-Pg', '--prof-gpu-id', default=config.PROFILING_GPU, type=int, choices=range(0,config.MAX_DEVICE_COUNT), 
                        metavar='N', help='which gpu to do the profiling on. Range = (0 to NUM_GPUS-1)')

## Baechi Settings
parser.add_argument('-sch', '--sch', default=config.SCHEME, type=str, choices=config.SCHEME_LIST, 
                        help='baechi algorithm scheme to be used - sct, etf or topo')

parser.add_argument('--topo-type', default=config.TOPO_TYPE, type=str, choices=['no_cap', 'with_cap'], 
                        help='If topo scheme is choosen, should a cap= (mem required)/(num devices) be applied?')

parser.add_argument('-At', '--assigner-type', default=config.ASSIGNER_TYPE, type=str, choices = config.ASSIGNER_LIST, 
                         help='Type of assigner to be used (see config.py for choices available)')

parser.add_argument('-Pf', '--perturb-factor', default=config.PERTURBATION_FACTOR, type=float, 
                         help='factor to perturb profiles by for sensitivity test. \
                                Profiles is perturbed by a uniform rv in (1 +/- PERTURBATION_FACTOR).\
                                Typically range to use -> 0 to 0.2')

parser.add_argument('-Mf', '--mem-factor', default=config.MEM_FACTOR , type=check_positive_float,  
                         help='Fraction of max_available_memory at each device allowed for placement.\
                               Used for incufficient memory experiments. Typically 0.3 to 1.0 ')

## Training setup
parser.add_argument('-b', '--batch-size', default=config.BATCH_SIZE , type=int, metavar='N', 
                        help='mini-batch size (default: 64)')

parser.add_argument('-m', '--model-name', default=config.MODEL_NAME, type=str, choices=config.MODEL_LIST, 
                         help='name of the model to be used')

parser.add_argument('-Mt', '--model-type', default='inception', type=str, choices=['inception', 'gnmt', 'transformer' ], 
                         help='type of model (for internal use only)')
                         
parser.add_argument('-rep', '--repetable',default=config.REPETABLE , type=int, choices=[0,1],
                    help='Some models can be initiated with fixed parameters, so that experiments are repetable')

parser.add_argument('-n', '--num-run', default=config.NUM_RUN, type=int, 
                         help='how many training steps to run in the experiment')

parser.add_argument('-Ci', '--capture-info', default=config.CAPTURE_INFO, type=str, choices = ['capture_profile', 'capture_memory', None],
                         help='what info must be recorded during the training runs: timing profile (using torch profiler), \
                         or memory usuage or none.')

parser.add_argument('-Cc', '--correctness-check', default=0, type=int, choices=[0,1], 
                         help='Set it to 1 to check correctness of the assigners wrapper. It assigns random\
                          device to each node. Set repetable also to 1 and compare the output generated by this run\
                          and from single_gpu_run.')

#-----------------------------------------------------------------------------------------------------
def main():
    args = parser.parse_args()

    #### TODO: Change if more types of 'transformer' or 'gnmt' models are added
    if args.model_name == 'gnmt':
        args.model_type = 'gnmt'
    elif args.model_name == 'transformer':
        args.model_type = 'transformer'
    else:
        args.model_type = 'inception'
    #####

    if args.mem_factor>1.0:
        _LOGGER.info("Given mem_factor value is {}. It has been capped at 1.0.".format(args.mem_factor))
        args.mem_factor = 1.0

    _LOGGER.info( "\n"+"*-"*30 +\
                "\nInput arguments:\n" +\
                ''.join(f'{k}=\t{v}\n' for k, v in vars(args).items()) + "\n" +\
                "*-"*30)
        
    if config.REVERSE_GRAPH_FLAG:
        _LOGGER.info("With reverse graph, reordering is not supported. Hence changing to only_stream_no_reordering")
        args.assigner_type = 'only_stream_no_reordering'

    if args.model_type == "gnmt" and args.assigner_type[0:5]!='gnmt':
        args.assigner_type = 'gnmt_assigner_stream'
        _LOGGER.info("Reordering does not work with gnmt yet. Because dropout layer is reused and \
                it can't simply be ignored when threads are used")

    if args.model_type == "gnmt":
        args.prof_gpu_id = 0
        _LOGGER.info("Profiling GPU changed to 0 for GNMT")
    # TODO: Some lateral inputs are defined when model is initilaized, which happens during profiling
    # Hence their device will be the prof_gpu. So made it zero to same some comm cost. Else SCT is very bad. Fix this


    if (args.model_type== "gnmt") and (config.ALLOW_GNMT==False):
        raise Exception("Baechi pytorch with GNMT has correctness and performance issues. Stopping the program.\
                        Please set ALLOW_GNMT manually to True in config.py to proceed")


    ##--*--*--*--*--*--*--*--*--*--*--*--*--*--* Starting Baechi *--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*

    ######################## Initializations ###########################
    _LOGGER.info("Starting Baechi Runtime.")
    utilsf.print_exp_settings(args)
    if args.model_type == "gnmt" and config.ALLOW_GNMT ==False:
        raise Exception("Baechi pytorch with GNMT has correctness and performance issues. Stopping the program")

    for dev in range(args.gpu_num):
        _LOGGER.info("Initializing device {} ".format(dev)) 
        torch.cuda.synchronize(dev)
        torch.cuda.set_per_process_memory_fraction(1.0, dev)

    _LOGGER.info("Getting the model {} from model nursery ".format(args.model_name))
    model, inp_size_single, opt_size, model_info, lr, inpt_factor = get_model(args.model_name, args.repetable)

    ############################ Proffiling #############################
    start_profile_time = time.time()
    profiled_model = profiler.Profiling(args.run_type, model, args.model_type, args.prof_gpu_id, args.prof_rounds,\
                                input_size = inp_size_single, output_size = opt_size, batch_size = args.batch_size,\
                                model_info = model_info)
    final_output = profiled_model.run()
    end_profile_time = time.time()
    _LOGGER.info("Profiling time =  {} sec".format((end_profile_time -start_profile_time) ))

    ###################### Build the models' Baechi graph ######################
    graph_info = {}
    graph_info['m_comm_time_vs_data_size'] = config.COMM_FACTOR*config.SLOPE_COMM_TIME 
    graph_info['b_comm_time_vs_data_size'] = config.INTERCEPT_COMM_TIME
    graph_info['perturb_factor']           = args.perturb_factor
    graph_info['node_weight_factor']       = config.NODE_WEIGHT_FACTOR 

    start_graph_time = time.time()
    return_graph, profiled_model = baechi_graph.build_graph(final_output, profiled_model, graph_info)
    utilsf.topological_sort(profiled_model)
    end_graph_time = time.time()
    _LOGGER.info("Graph creation time =  {} sec".format(end_graph_time -start_graph_time ))

    ###################### Build the device graph ############################

    device_list={} 
    for dev in range(config.MAX_DEVICE_COUNT):
        device_list[dev] = {'name': '/job:localhost/replica:0/task:0/device:XLA_GPU:'+str(dev),\
                            'memory_size': args.mem_factor*config.MAX_AVAILABLE_MEMORY , 'type': ''}
    available_devices     = range(args.gpu_num)
    available_device_list = {k:device_list[k] for k in available_devices}
    DEVICE_GRAPH_MULTIPLE = baechi_graph.create_device_graph(available_device_list)
    _LOGGER.info("Device graph created with {} gpus. Details:".format(args.gpu_num))
    for dev in available_device_list:
        _LOGGER.info("Device {}. Memory = {}".format(dev, 
                    utilsf.humanize_num_bytes(available_device_list[dev]['memory_size']) ))


    ############ Feed the model and device graphs to choosen placement Baechi algorithm ############# 

    placement_start_time = time.time()
    if args.sch == "sct":
        _LOGGER.info("Implementing m-SCT")
        placed_op_graph = m_sct(return_graph, DEVICE_GRAPH_MULTIPLE)
        
    elif args.sch == "etf":
        _LOGGER.info("Implementing m-ETF")
        placed_op_graph = m_etf(return_graph, DEVICE_GRAPH_MULTIPLE)
        
    elif args.sch == "topo":
        _LOGGER.info("Implementing m-TOPO")
        if args.topo_type =="no_cap":
            _LOGGER.info("Topo without cap.")
            placed_op_graph = m_topo(return_graph, DEVICE_GRAPH_MULTIPLE, uniform = False)
        else:
            _LOGGER.info("Topo with cap.")
            placed_op_graph = m_topo(return_graph, DEVICE_GRAPH_MULTIPLE, uniform = True)
    placement_end_time = time.time()

    _LOGGER.info("Placement time = {}".format(placement_end_time - placement_start_time) )

    utilsf.copy_p(return_graph, profiled_model)
    utilsf.print_assigned_graph(return_graph)

    ################################################################################################################
    # Post placement adjustmet incase of weight sharing (transformers only)
    if args.model_type == "transformer":
        ## Source embedding and final project are moved to same device and target embedding depending on the flags
        if model_info['trg_emb_prj_weight_sharing']:    
            node_prj = profiled_model.model.__dict__['_modules']['trg_word_prj']
            node_dec = profiled_model.model.__dict__['_modules']['decoder'].__dict__['_modules']['trg_word_emb']
            profiled_model.sub_module_nodes[id(node_dec)].p = profiled_model.sub_module_nodes[id(node_prj)].p 
            _LOGGER.info( "Source embedding and final projection layer weight sharing status= {}".format\
                            (profiled_model.model.trg_word_prj.weight == profiled_model.model.decoder.trg_word_emb.weight) )

        if model_info['emb_src_trg_weight_sharing']:
            node_src = profiled_model.model.__dict__['_modules']['encoder'].__dict__['_modules']['src_word_emb']
            node_dec = profiled_model.model.__dict__['_modules']['decoder'].__dict__['_modules']['trg_word_emb']
            profiled_model.sub_module_nodes[id(node_src)].p = profiled_model.sub_module_nodes[id(node_dec)].p
            _LOGGER.info( "Source and target embedding layer weight sharing status= {}".format\
                            (profiled_model.model.encoder.src_word_emb.weight == profiled_model.model.decoder.trg_word_emb.weight) )

    ##################################### For correctness check only##########################################
    ### Randomly assigns devices to nodes of the graph. The fina output of this run,
    ### should match a single-gpu run for the same setting. Single gpu run can be
    ### performed through train_singlegpu.py or by setting gpu_num=1 in train_baechi.py
    if args.correctness_check:
        _LOGGER.info("NOTE:***Correctness check has been enabled. Thus assigning random devices to nodes")
        import random
        random.seed(0)
        rand_list=[]
        for node_id in profiled_model.sub_module_nodes: 
            profiled_model.sub_module_nodes[node_id].p = random.randint(0,args.gpu_num-1)
            rand_list.append(profiled_model.sub_module_nodes[node_id].p)

    ################################# For printing info about the generated palcement ##############################
    ## Also gets gpu's of first and final node - needed to move inputs and labels to appropriate devices 
    first_gpu = -1
    print_parent_info = False 
    print("Printing placed graph information:")
    for node_id in profiled_model.sub_module_nodes: 
        print("Module:", profiled_model.sub_module_nodes[node_id].module)
        print("id:", id(profiled_model.sub_module_nodes[node_id].module))
        curr_gpu_id = profiled_model.sub_module_nodes[node_id].p
        assert (curr_gpu_id < args.gpu_num) , "Invalid device allotted to a node" 
        print("Assigned GPU:", curr_gpu_id)
        print("Execution order:",profiled_model.sub_module_nodes[node_id].execution_order)
        print("Topo order:",profiled_model.sub_module_nodes[node_id].topo_order)
        if print_parent_info:
            print("Parent modules:")
            for par in profiled_model.sub_module_nodes[node_id].parent:
                print("\t", profiled_model.sub_module_nodes[par].module, id(profiled_model.sub_module_nodes[par].module) )
            print()
            print("Children modules:")
            for child in profiled_model.sub_module_nodes[node_id].children:
                print("\t", profiled_model.sub_module_nodes[child].module, id(profiled_model.sub_module_nodes[child].module) )
        if first_gpu < 0:
            first_gpu = curr_gpu_id
        print("-"*40)
        print()
    final_gpu = curr_gpu_id

    ##### Check topological order allotment is correct ####
    topo_list=[]
    for node_id in profiled_model.sub_module_nodes:
        topo_list.append(profiled_model.sub_module_nodes[node_id].topo_order)
        topo_list.sort()  
    for i in range(len(topo_list)-1):
        assert topo_list[i+1] == topo_list[i]+1, "Something wrong"

    ######################### Training/Infference according to  the generated palcement #########################################
    ##--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*--*

    ## Set memory limits on each allowed device for this process
    for dev in range(args.gpu_num):
        torch.cuda.set_per_process_memory_fraction(args.mem_factor*1.0, dev)

    ################# Distribute model across devices acc to the generatted placement ###################
    assigner = assigners.assigner(profiled_model, args.assigner_type, args)
    for dev in range(args.gpu_num):
        torch.cuda.synchronize(dev) 

    #If there are no parameters in model, using optimizer throws error (use for some minor tests)
    use_optimizer = (len(list(model.parameters()))>0) 

    ############################################## Setup the (random) Inputs ####################################################

    _LOGGER.info("Setting up inputs:")

    #### GNMT models
    if args.model_type  == 'gnmt':
        inp_enc_data = torch.randint(model_info['vocab_size'], (args.num_run, args.batch_size, model_info['max_sequence_length']))
        inp_dec_data = torch.randint(model_info['vocab_size'], (args.num_run,args.batch_size, model_info['max_sequence_length']))
        inp_seq_len_data = torch.sort(torch.randint(model_info['min_sequence_length'],\
                                        model_info['max_sequence_length'], (args.num_run,args.batch_size)),\
                                        descending=True)[0]
        labels_data = torch.empty(args.num_run, args.batch_size, model_info['vocab_size'], dtype=torch.long).random_(2)
        if args.run_type == "training":
            optimizer = optim.SGD(profiled_model.model.parameters(), lr); 
            criterion = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')   

    #### Transformer models
    elif args.model_name  == 'transformer': # only training
        assert (args.run_type == "training"), "Inference for transformer not handled" #TODO
        torch.random.manual_seed(0)
        inp_data = [None for _ in range(len(inp_size_single))]
        for i in range(len(inp_size_single)): 
            if inp_size_single[i][0] == "int":
                inp_data[i] = torch.randint(model_info['vocab_size'], (args.num_run, args.batch_size,) + inp_size_single[i][1])
            else:
                inp_data[i] = torch.randn((args.num_run, args.batch_size,) + inp_size_single[i])
        labels_data = torch.randn( (args.num_run, args.batch_size,) + opt_size)

        if use_optimizer:
            optimizer = optim.SGD(profiled_model.model.parameters(), lr = 0.0001); optimizer.zero_grad()
        criterion = nn.MSELoss()   

    else:
    ### Others (Vision models)
        inp_size = (args.batch_size,) + inp_size_single
        opt_size_tup = (args.batch_size, opt_size)
        random_factor = [-2,-1,1,2]
        
        if args.repetable == 1:
            ## intialized all input to ones
            inp_data   = torch.ones((args.num_run,) + inp_size)*(inpt_factor)
            labels_data = (torch.ones((args.num_run,) +opt_size_tup)*0.5)
            ### intialized all input to repetable randn
            #torch.random.manual_seed(0)
            #inp   = torch.randn((args.num_run,) + inp_size)*(inpt_factor)
        else:
            ## intialized all inputs randomly
            inp_data   = torch.randn((args.num_run,) + inp_size)*(inpt_factor)
            labels_data = (torch.randn((args.num_run,) +opt_size_tup)*0.5)
            
        for run_no in range(args.num_run): 
            ## To make repetable inputs different accros consecutive run to prevent
            ## correctness bugs due to tensor caching
            inp_data[run_no]   = inp_data[run_no]*random_factor[run_no%4]

        if args.run_type == "training":
            optimizer = optim.SGD(profiled_model.model.parameters(), lr); 
            criterion = nn.MSELoss()


    ## Choose what info must be recorded during runs
    context = utilsf.NullContextManager() #change
    if args.capture_info == 'capture_profile':
        ## This may slow down the training steps
        from torch.profiler import profile, record_function, ProfilerActivity
        context = profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA])
    elif args.capture_info == 'capture_memory':
        context = utilsf.TorchTracemalloc_new(-1,False)

    ########################################### Training runs ################################################

    if args.run_type == "training":

        out_sum = 0

        times = []
        times_f =[]
        times_b =[]
        
        for run_no in range(args.num_run):
            print("Run number:", run_no)
            with context as ctx:
                start = time.time()
                #--------- move input and labels -----------
                if args.model_type == "inception":
                    inp = inp_data[run_no].to(first_gpu)# althought the modified forward can get inputs to required gpu
                                                        # there may be preprocessing which will be fast if input already on gpu
                                                        # eg: _transform in inception_v3
                    labels = labels_data[run_no].to(final_gpu)
                elif args.model_type == "gnmt":
                    inp_enc = inp_enc_data[run_no].to(first_gpu)
                    inp_dec = inp_dec_data[run_no].to(first_gpu)
                    inp_seq_len = inp_seq_len_data[run_no].to(first_gpu)
                    labels = labels_data[run_no].to(final_gpu)
                elif args.model_type == "transformer":
                    inp = [None for _ in range(len(inp_size_single))]
                    for i in range(len(inp_size_single)):
                        inp[i] = inp_data[i][run_no]
                        if inp_size_single[i][0] != "int":
                            inp[i].requires_grad = True
                    inp = tuple(inp)
                    labels = labels_data[run_no].to(final_gpu)
                
                if use_optimizer:
                    optimizer.zero_grad()
                
                #--------------- forward ------------------
                f_start = time.time()
                if args.model_type == "inception":
                    output = profiled_model.model(inp)
                elif args.model_type == "gnmt":   
                    output = profiled_model.model(inp_enc, inp_seq_len, inp_dec)
                elif args.model_type == "transformer":
                    output = profiled_model.model(*inp)
                
                output = assigner.wait_for_threads(output)
                if args.model_type == "transformer":
                    if isinstance(output, tuple) :
                        output = output[0]

                # TODO: Is this sync required?
                for dev in range(args.gpu_num):
                    torch.cuda.synchronize(dev)

                assigner.clear_records()
                    
                #--------------- backward ------------------
                b_start = time.time()
                loss = criterion(output, labels)
                loss.backward(loss)
                if use_optimizer:
                    optimizer.step()
                for dev in range(args.gpu_num):
                    torch.cuda.synchronize(dev)
                end = time.time()

            # for correctness_check only  
            op_clone = output.detach().clone()  
            out_sum = out_sum + torch.sum(op_clone)
            del op_clone

            times.append(1000*(end-start))
            times_f.append(1000*(b_start-f_start))
            times_b.append(1000*(end-b_start))
            print("-*"*40)

        if args.capture_info == 'capture_profile':
            ctx.export_chrome_trace("traces/trace_baechi.json")
        if args.capture_info == 'capture_memory':
            used_mem = ctx.end
            peaked_mem = ctx.peak

        gpu_time = np.mean(times[int(args.num_run/4):])
        gpu_time_f = np.mean(times_f[int(args.num_run/4):])
        gpu_time_b = np.mean(times_b[int(args.num_run/4):])
        print("Mean time taken:", gpu_time)
        print("Mean forward time taken:", gpu_time_f)
        print("Mean backward time taken:", gpu_time_b)
        
        print("Output_sum:", out_sum)
        print()

    ########################################### Inference Runs ##############################################

    if args.run_type == "inference":

        out_sum = 0
        times = []

        for run_no in range(Nrun):
            print("Run number:", run_no);print("*"*30)

            with context as ctx:
                if args.model_type == "inception":
                    inp = inp_data[run_no].to(first_gpu)
                    #labels = labels_data[run_no].to(final_gpu)
                elif args.model_type == "gnmt":
                    inp_enc = inp_enc_data[run_no].to(first_gpu)
                    inp_dec = inp_dec_data[run_no].to(first_gpu)
                    inp_seq_len = inp_seq_len_data[run_no].to(first_gpu)
                    #labels = labels_data[run_no].to(final_gpu)
                else:
                    raise Exception("Inference not handled for your model type")
                    

                start = time.time()
                with torch.no_grad():
                    output = profiled_model.model(inp)
                    outptut = assigner.wait_for_threads(output)

                for dev in range(args.gpu_num):
                    torch.cuda.synchronize(dev)
                end = time.time()

                op_clone = output.clone().detach()
                out_sum = out_sum + torch.sum(op_clone)
                del op_clone

                assigner.clear_records()
                times.append(1000*(end-start))
                
        if capture_info == 'capture_profile':
            ctx.export_chrome_trace("traces/trace_baechi.json")
        if capture_info == 'capture_memory':
            used_mem = ctx.end
            peaked_mem = ctx.peak

        gpu_time = np.mean(times[int(args.num_run/4):])
        print("Mean time taken:", gpu_time)
        print()
        output = out_sum

    ################################################################################################################

    if args.capture_info == 'capture_memory':
        utilsf.record_result('baechi', args, gpu_time, used_mem, peaked_mem)

    ################################################################################################################
    #### To clear memory after training iff the process does not end here (eg: if executed as a jupyter notebook)
    clear_memory = False
    if clear_memory:
        import gc
        del model
        try:
            del inp
        except:
            pass
        try:
            del inp_enc_data, inp_dec_data, inp_seq_len
        except:
            pass
        del output

        del labels
        del optimizer
        del loss
        del profiled_model
        del assigner
        del return_graph
        del placed_op_graph

        del inp_size_single, opt_size, lr, inpt_factor 
        del model_nursery
        sys.modules.pop('model_nursery')

        utilsf.delTensors()
        gc.collect()
        torch.cuda.empty_cache()

if __name__ == '__main__':
    main()